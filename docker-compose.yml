version: "3.8"

services:
  # Notex Application
  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: notex
    environment:
      # Server
      - SERVER_HOST=0.0.0.0
      - SERVER_PORT=8080

      # LLM (choose one)
      # Fill your OpenAI API key or use Ollama
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL:-}
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4o-mini}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-text-embedding-3-small}
      - EMBEDDING_PROVIDER=${EMBEDDING_PROVIDER:-google}
      - IMAGE_MODEL=${IMAGE_MODEL:-gemini-2.5-flash-image-preview}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY:-}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.2}
      - CHAT_PROVIDER=${CHAT_PROVIDER:-openai}
      - CHAT_MODEL=${CHAT_MODEL:-qwen3-max}

      # Features
      - ENABLE_MARKITDOWN=${ENABLE_MARKITDOWN:-true}
      - ENABLE_PODCAST=${ENABLE_PODCAST:-true}
      - PODCAST_VOICE=${PODCAST_VOICE:-alloy}
      - ALLOW_DELETE=${ALLOW_DELETE:-true}
      - ALLOW_MULTIPLE_NOTES_OF_SAME_TYPE=${ALLOW_MULTIPLE_NOTES_OF_SAME_TYPE:-true}

      # RAG
      - MAX_SOURCES=${MAX_SOURCES:-5}
      - MAX_CONTEXT_LENGTH=${MAX_CONTEXT_LENGTH:-128000}
      - CHUNK_SIZE=${CHUNK_SIZE:-1000}
      - CHUNK_OVERLAP=${CHUNK_OVERLAP:-200}

      # Vector Store (Default to SQLite for easy setup)
      - VECTOR_STORE_TYPE=sqlite
      - SQLITE_PATH=/data/vector.db

      # Store (Metadata)
      - STORE_TYPE=sqlite
      - STORE_PATH=/data/checkpoints.db

    ports:
      - "8080:8080"
    volumes:
      - ./data:/data
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped

  # Ollama (Optional, if you want to run LLM locally in container)
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: notex-ollama
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama-data:/root/.ollama
  #   restart: unless-stopped

volumes:
  app-data:
